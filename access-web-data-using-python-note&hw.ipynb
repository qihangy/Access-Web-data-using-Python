{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week3\n",
    "# Using urllib in Python\n",
    "\n",
    "Since HTTP is so common, we have a library that does all the socket work for us and makes web pages look like a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- When we talk to an external resource like a network socket we send bytes, so we need to encode Python 3 strings into a given character encoding <br/>\n",
    "-- When we read data from an external resource, we must decode it based on the character set so it is properly represented in Python 3 as a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 400 Bad Request\r\n",
      "Date: Tue, 21 Apr 2020 17:18:01 GMT\r\n",
      "Server: Apache/2.4.18 (Ubuntu)\r\n",
      "Content-Length: 308\r\n",
      "Connection: close\r\n",
      "Content-Type: text/html; charset=iso-8859-1\r\n",
      "\r\n",
      "<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n",
      "<html><head>\n",
      "<title>400 Bad Request</title>\n",
      "</head><body>\n",
      "<h1>Bad Request</h1>\n",
      "<p>Your browser sent a request that this server could not understand.<br />\n",
      "</p>\n",
      "<hr>\n",
      "<address>Apache/2.4.18 (Ubuntu) Server at do1.dr-chuck.com Port 80</address>\n",
      "</body></html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\n\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    print(data.decode())\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to run a for loop for line in this handle. \n",
    "So that's going to iterate through all the lines of this URL. \n",
    "So that's going to open the URL, read the data, and iterate with a for loop once through each line. \n",
    "Now, this line iteration is actually a byte array, not a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "\n",
    "counts = dict()\n",
    "for line in fhand:\n",
    "    words = line.decode().split()\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "print(counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and        3\n",
       "is         3\n",
       "the        3\n",
       "sun        2\n",
       "kill       1\n",
       "It         1\n",
       "window     1\n",
       "soft       1\n",
       "through    1\n",
       "light      1\n",
       "envious    1\n",
       "already    1\n",
       "sick       1\n",
       "Arise      1\n",
       "with       1\n",
       "Who        1\n",
       "grief      1\n",
       "east       1\n",
       "But        1\n",
       "breaks     1\n",
       "what       1\n",
       "fair       1\n",
       "Juliet     1\n",
       "pale       1\n",
       "moon       1\n",
       "yonder     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternative way of writing the previoud code\n",
    "import pandas as pd\n",
    "import functools\n",
    "import itertools\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "s = []\n",
    "for line in fhand:\n",
    "    s.append(line.decode().strip().split())\n",
    "\n",
    "s = list(itertools.chain.from_iterable(s))\n",
    "pd.value_counts(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the HyperText Transport Protocol <br/>\n",
    "\n",
    "You are to retrieve the following document using the HTTP protocol in a way that you can examine the HTTP Response headers.\n",
    "\n",
    "http://data.pr4e.org/intro-short.txt  <br/>\n",
    "\n",
    "There are three ways that you might retrieve this web page and look at the response headers:\n",
    "\n",
    "-- Preferred: Modify the socket1.py program to retrieve the above URL and print out the headers and data. Make sure to change the code to retrieve the above URL - the values are different for each URL. <br/>\n",
    "-- Open the URL in a web browser with a developer console or FireBug and manually examine the headers that are returned. <br/>\n",
    "-- Use the telnet program as shown in lecture to retrieve the headers and content. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r\n",
      "Date: Tue, 21 Apr 2020 17:17:56 GMT\r\n",
      "Server: Apache/2.4.18 (Ubuntu)\r\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\r\n",
      "ETag: \"a7-54f6609245537\"\r\n",
      "Accept-Ranges: bytes\r\n",
      "Content-Length: 167\r\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\r\n",
      "Pragma: no-cache\r\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\r\n",
      "Connection: close\r\n",
      "Content-Type: text/plain\r\n",
      "\r\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(),end='')\n",
    "\n",
    "mysock.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>The First Page</h1>\n",
      "<p>\n",
      "If you like, you can switch to the\n",
      "<a href=\"http://www.dr-chuck.com/page2.htm\">\n",
      "Second Page</a>.\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "fhand = urllib.request.urlopen('http://www.dr-chuck.com/page1.htm')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we're going to talk about how you can more efficiently take apart the HTML and look for various things and print those things out. Because it turns out that HTML is so ugly and so inconsistent that things like regular "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 \n",
    "# Parsing Web Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this, you can install BeautifulSoup\n",
    "# https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# Or download the file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter -http://www.dr-chuck.com/page2.htm\n",
      "page1.htm\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = input('Enter -')\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " BeautifulSoup library to retrieve and parse HTML and pull out anchor tags, which is really sort of the beginning of a browser. I mean, a beginning of a web crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://www.dr-chuck.com/page2.htm\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://www.dr-chuck.com/page2.htm\n",
      "page1.htm\n"
     ]
    }
   ],
   "source": [
    "# To run this, download the BeautifulSoup zip file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "x = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "for line in x:\n",
    "    print(line.decode().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.dr-chuck.com\">here</a></p>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "ctx = '<p>Please click <a href=\"http://www.dr-chuck.com\">here</a></p>'\n",
    "re.findall('http://.*', ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAG: <a href=\"page1.htm\">\n",
      "First Page</a>\n",
      "URL: page1.htm\n",
      "Contents: \n",
      "First Page\n",
      "Attrs: {'href': 'page1.htm'}\n"
     ]
    }
   ],
   "source": [
    "# To run this, download the BeautifulSoup zip file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = 'http://www.dr-chuck.com/page2.htm'\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    print('TAG:', tag)\n",
    "    print('URL:', tag.get('href', None))\n",
    "    print('Contents:', tag.contents[0])\n",
    "    print('Attrs:', tag.attrs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Scraping Numbers from HTML using BeautifulSoup In this assignment you will write a Python program similar to http://www.py4e.com/code3/urllink2.py. The program will use urllib to read the HTML from the data files below, and parse the data, extracting numbers and compute the sum of the numbers in the file.\n",
    "\n",
    "-- We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/comments_42.html (Sum=2553) <br/>\n",
    "Actual data: http://py4e-data.dr-chuck.net/comments_448547.html (Sum ends with 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2501"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "def words_sum(url):\n",
    "    \n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tags = soup('span')\n",
    "\n",
    "    Sum = 0\n",
    "    for tag in tags:\n",
    "        Sum = Sum + int(tag.contents[0])\n",
    "    \n",
    "    return Sum\n",
    "\n",
    "url = 'http://py4e-data.dr-chuck.net/comments_448547.html'\n",
    "words_sum(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Links in Python\n",
    "\n",
    "In this assignment you will write a Python program that expands on http://www.py4e.com/code3/urllinks.py. The program will use urllib to read the HTML from the data files below, extract the href= vaues from the anchor tags, scan for a tag that is in a particular position relative to the first name in the list, follow that link and repeat the process a number of times and report the last name you find.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the name for your testing and the other is the actual data you need to process for the assignment\n",
    "\n",
    "Sample problem: Start at http://py4e-data.dr-chuck.net/known_by_Fikret.html <br/>\n",
    "Find the link at position 3 (the first name is 1). Follow that link. Repeat this process 4 times. The answer is the last name that you retrieve.<br/>\n",
    "Sequence of names: Fikret Montgomery Mhairade Butchi Anayah <br/>\n",
    "Last name in sequence: Anayah <br/>\n",
    "\n",
    "Actual problem: Start at: http://py4e-data.dr-chuck.net/known_by_Rhianne.html <br/>\n",
    "Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.<br/>\n",
    "Hint: The first character of the name of the last page that you will load is: A \n",
    "\n",
    "-- Strategy <br/>\n",
    "The web pages tweak the height between the links and hide the page after a few seconds to make it difficult for you to do the assignment without writing a Python program. But frankly with a little effort and patience you can overcome these attempts to make it a little harder to complete the assignment without writing a Python program. But that is not the point. The point is to write a clever Python program to solve the program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this, download the BeautifulSoup zip file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "def problem_search(url, count, position):\n",
    "    \n",
    "    # count = 5\n",
    "    \n",
    "    def find_name(url,position):      \n",
    "\n",
    "        html = urllib.request.urlopen(url, context=ctx).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Retrieve all of the anchor tags\n",
    "        tags = soup('a')\n",
    "        url_new = tags[position-1].get('href', None)\n",
    "\n",
    "        return url_new\n",
    "\n",
    "    def name(count):\n",
    "        \n",
    "        n = 1\n",
    "        temp = url \n",
    "\n",
    "        while n < count+1:\n",
    "            temp = find_name(temp,position)            \n",
    "            n += 1\n",
    "                \n",
    "        return temp[:-5].split('_')[-1]\n",
    "    \n",
    "    output = name(count)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anayah'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://py4e-data.dr-chuck.net/known_by_Fikret.html'\n",
    "count = 4\n",
    "position = 3 \n",
    "\n",
    "problem_search(url,count,position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ammer'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://py4e-data.dr-chuck.net/known_by_Rhianne.html'\n",
    "count = 7\n",
    "position = 18 \n",
    "\n",
    "problem_search(url,count,position)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5\n",
    "# Extensible Markup Language (XML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Chuck\n",
      "Attr: yes\n",
      "Phone number: +17343034456\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "data = '''\n",
    "<person>\n",
    "  <name>Chuck</name>\n",
    "  <phone type=\"intl\">\n",
    "    +1 734 303 4456\n",
    "  </phone>\n",
    "  <email hide=\"yes\" />\n",
    "</person>'''\n",
    "\n",
    "tree = ET.fromstring(data)\n",
    "print('Name:', tree.find('name').text)\n",
    "print('Attr:', tree.find('email').get('hide'))\n",
    "print('Phone number:',('+' + ''.join(re.findall('[0-9]+',tree.find('phone').text))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User count: 2\n",
      "Name Chuck\n",
      "Id 001\n",
      "Attribute 2\n",
      "Name Brent\n",
      "Id 009\n",
      "Attribute 7\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "input = '''\n",
    "<stuff>\n",
    "  <users>\n",
    "    <user x=\"2\">\n",
    "      <id>001</id>\n",
    "      <name>Chuck</name>\n",
    "    </user>\n",
    "    <user x=\"7\">\n",
    "      <id>009</id>\n",
    "      <name>Brent</name>\n",
    "    </user>\n",
    "  </users>\n",
    "</stuff>'''\n",
    "\n",
    "stuff = ET.fromstring(input) #stuff is a tree structured information \n",
    "\n",
    "lst = stuff.findall('users/user')\n",
    "print('User count:', len(lst))\n",
    "\n",
    "for item in lst:\n",
    "    print('Name', item.find('name').text)\n",
    "    print('Id', item.find('id').text)\n",
    "    print('Attribute', item.get('x'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Chuck', '7')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst[0].find('name').text, lst[1].get('x')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Extracting Data from XML\n",
    "\n",
    "-- In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/geoxml.py. The program will prompt for a URL, read the XML data from that URL using urllib and then parse and extract the comment counts from the XML data, compute the sum of the numbers in the file.\n",
    "\n",
    "-- We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/comments_42.xml (Sum=2553) <br/>\n",
    "Actual data: http://py4e-data.dr-chuck.net/comments_448549.xml (Sum ends with 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2725"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request as ur\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "url = 'http://py4e-data.dr-chuck.net/comments_448549.xml'\n",
    "#print('Retrieving', url)\n",
    "\n",
    "def calc_Sum(url):\n",
    "    \n",
    "    xml = ur.urlopen(url).read()\n",
    "    #print('Retrieved', len(xml), 'characters')\n",
    "\n",
    "    tree = et.fromstring(xml)\n",
    "    Length_tree = len(tree.findall('.//count'))\n",
    "\n",
    "    Sum = 0\n",
    "\n",
    "    for i in range(Length_tree):\n",
    "        Sum = Sum + int(tree.findall('.//count')[i].text)\n",
    "    \n",
    "    return Sum\n",
    "\n",
    "calc_Sum(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6\n",
    "# JavaScript Object Notation (JSON)\n",
    "-- The code is simpler \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Chuck\n",
      "Hide: yes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = '''{\n",
    "  \"name\" : \"Chuck\",\n",
    "  \"phone\" : {\n",
    "    \"type\" : \"intl\",\n",
    "    \"number\" : \"+1 734 303 4456\"\n",
    "   },\n",
    "   \"email\" : {\n",
    "     \"hide\" : \"yes\"\n",
    "   }\n",
    "}'''\n",
    "\n",
    "info = json.loads(data)\n",
    "print('Name:',info[\"name\"])\n",
    "print('Hide:',info[\"email\"][\"hide\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User count: 2\n",
      "Name Chuck\n",
      "Id 001\n",
      "Attribute 2\n",
      "Name Chuck\n",
      "Id 009\n",
      "Attribute 7\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "input = '''[\n",
    "  { \"id\" : \"001\",\n",
    "    \"x\" : \"2\",\n",
    "    \"name\" : \"Chuck\"\n",
    "  } ,\n",
    "  { \"id\" : \"009\",\n",
    "    \"x\" : \"7\",\n",
    "    \"name\" : \"Chuck\"\n",
    "  }\n",
    "]'''\n",
    "\n",
    "info = json.loads(input)\n",
    "print('User count:', len(info))\n",
    "for item in info:\n",
    "    print('Name', item['name'])\n",
    "    print('Id', item['id'])\n",
    "    print('Attribute', item['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '001', 'x': '2', 'name': 'Chuck'},\n",
       " {'id': '009', 'x': '7', 'name': 'Chuck'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- that's what json format looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'OK',\n",
       " 'results': [{'geometry': {'location_type': 'APPROXIMATE',\n",
       "    'location': {'lat': 42.2808256, 'lng': -83.7430378}},\n",
       "   'address_components': [{'long_name': 'Ann Arbor',\n",
       "     'types': ['locality', 'political'],\n",
       "     'short_name': 'Ann Arbor'}],\n",
       "   'formatted_address': 'Ann Arbor, MI, USA',\n",
       "   'types': ['locality', 'political']}]}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"status\": \"OK\",\n",
    "     \"results\": [\n",
    "        {\n",
    "            \"geometry\": {\n",
    "                \"location_type\": \"APPROXIMATE\",\n",
    "                 \"location\": {\n",
    "                    \"lat\": 42.2808256,\n",
    "                     \"lng\": -83.7430378\n",
    "                }\n",
    "            },\n",
    "            \"address_components\": [\n",
    "                {\n",
    "                    \"long_name\": \"Ann Arbor\",\n",
    "                     \"types\": [\n",
    "                        \"locality\",\n",
    "                         \"political\"\n",
    "                    ],\n",
    "                    \"short_name\": \"Ann Arbor\"\n",
    "                }\n",
    "             ],\n",
    "             \"formatted_address\": \"Ann Arbor, MI, USA\",\n",
    "             \"types\": [\n",
    "                \"locality\",\n",
    "                \"political\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- 不一定能跑出来的的Google API version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter location: Ann Arbor, MI\n",
      "Retrieving http://maps.googleapis.com/maps/api/geocode/json?address=Ann+Arbor%2C+MI\n",
      "Retrieved 237 characters\n",
      "==== Failure To Retrieve ====\n",
      "{\n",
      "   \"error_message\" : \"You must use an API key to authenticate each request to Google Maps Platform APIs. For additional information, please refer to http://g.co/dev/maps-no-account\",\n",
      "   \"results\" : [],\n",
      "   \"status\" : \"REQUEST_DENIED\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "\n",
    "serviceurl = 'http://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1: break\n",
    "\n",
    "    url = serviceurl + urllib.parse.urlencode({'address': address})\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "\n",
    "    if not js or 'status' not in js or js['status'] != 'OK':\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)      \n",
    "        break\n",
    "        \n",
    "\n",
    "    lat = js[\"results\"][0][\"geometry\"][\"location\"][\"lat\"]\n",
    "    lng = js[\"results\"][0][\"geometry\"][\"location\"][\"lng\"]\n",
    "    print('lat', lat, 'lng', lng)\n",
    "    location = js['results'][0]['formatted_address']\n",
    "    print(location)\n",
    "    break\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "########################\n",
    "# input:Ann Arbor, MI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- 一定能跑出来的的Google API version, up to daily limits 2500 request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter location: Ann Arbor, MI\n",
      "Retrieving http://py4e-data.dr-chuck.net/json?address=Ann+Arbor%2C+MI&key=42\n",
      "Retrieved 1736 characters\n",
      "{\n",
      "    \"results\": [\n",
      "        {\n",
      "            \"address_components\": [\n",
      "                {\n",
      "                    \"long_name\": \"Ann Arbor\",\n",
      "                    \"short_name\": \"Ann Arbor\",\n",
      "                    \"types\": [\n",
      "                        \"locality\",\n",
      "                        \"political\"\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"long_name\": \"Washtenaw County\",\n",
      "                    \"short_name\": \"Washtenaw County\",\n",
      "                    \"types\": [\n",
      "                        \"administrative_area_level_2\",\n",
      "                        \"political\"\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"long_name\": \"Michigan\",\n",
      "                    \"short_name\": \"MI\",\n",
      "                    \"types\": [\n",
      "                        \"administrative_area_level_1\",\n",
      "                        \"political\"\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"long_name\": \"United States\",\n",
      "                    \"short_name\": \"US\",\n",
      "                    \"types\": [\n",
      "                        \"country\",\n",
      "                        \"political\"\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"formatted_address\": \"Ann Arbor, MI, USA\",\n",
      "            \"geometry\": {\n",
      "                \"bounds\": {\n",
      "                    \"northeast\": {\n",
      "                        \"lat\": 42.3239728,\n",
      "                        \"lng\": -83.6758069\n",
      "                    },\n",
      "                    \"southwest\": {\n",
      "                        \"lat\": 42.222668,\n",
      "                        \"lng\": -83.799572\n",
      "                    }\n",
      "                },\n",
      "                \"location\": {\n",
      "                    \"lat\": 42.2808256,\n",
      "                    \"lng\": -83.7430378\n",
      "                },\n",
      "                \"location_type\": \"APPROXIMATE\",\n",
      "                \"viewport\": {\n",
      "                    \"northeast\": {\n",
      "                        \"lat\": 42.3239728,\n",
      "                        \"lng\": -83.6758069\n",
      "                    },\n",
      "                    \"southwest\": {\n",
      "                        \"lat\": 42.222668,\n",
      "                        \"lng\": -83.799572\n",
      "                    }\n",
      "                }\n",
      "            },\n",
      "            \"place_id\": \"ChIJMx9D1A2wPIgR4rXIhkb5Cds\",\n",
      "            \"types\": [\n",
      "                \"locality\",\n",
      "                \"political\"\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"status\": \"OK\"\n",
      "}\n",
      "lat 42.2808256 lng -83.7430378\n",
      "Ann Arbor, MI, USA\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "# api_key = 'AIzaSy___IDByT70'\n",
    "# https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "else :\n",
    "    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1: break\n",
    "\n",
    "    parms = dict()\n",
    "    parms['address'] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "\n",
    "    if not js or 'status' not in js or js['status'] != 'OK':\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        continue\n",
    "\n",
    "    print(json.dumps(js, indent=4))\n",
    "\n",
    "    lat = js['results'][0]['geometry']['location']['lat']\n",
    "    lng = js['results'][0]['geometry']['location']['lng']\n",
    "    print('lat', lat, 'lng', lng)\n",
    "    location = js['results'][0]['formatted_address']\n",
    "    print(location)\n",
    "    break\n",
    "    \n",
    "    \n",
    "\n",
    "########################\n",
    "# input:Ann Arbor, MI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6 Part 1 \n",
    "-- Extracting Data from JSON\n",
    "\n",
    "In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/json2.py. The program will prompt for a URL, read the JSON data from that URL using urllib and then parse and extract the comment counts from the JSON data, compute the sum of the numbers in the file and enter the sum below:\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/comments_42.json (Sum=2553) <br/>\n",
    "Actual data: http://py4e-data.dr-chuck.net/comments_448550.json (Sum ends with 39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "\n",
    "def get_json(url):\n",
    "\n",
    "    uh = urllib.request.urlopen(url)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    js = json.loads(data)\n",
    "    Sum = 0 \n",
    "\n",
    "    for i in range(len(js['comments'])):\n",
    "        Sum += int(js['comments'][i]['count'])\n",
    "        \n",
    "    return Sum \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 2711 characters\n",
      "2553\n",
      "Retrieved 2734 characters\n",
      "2839\n"
     ]
    }
   ],
   "source": [
    "# this is a sample question\n",
    "url = 'http://py4e-data.dr-chuck.net/comments_42.json'\n",
    "print(get_json(url))\n",
    "\n",
    "# this is a test question\n",
    "url = 'http://py4e-data.dr-chuck.net/comments_448550.json'\n",
    "print(get_json(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6 Part 2\n",
    "-- In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/geojson.py. The program will prompt for a location, contact a web service and retrieve JSON for the web service and parse that data, and retrieve the first place_id from the JSON. A place ID is a textual identifier that uniquely identifies a place as within Google Maps.\n",
    "\n",
    "-- You can test to see if your program is working with a location of \"South Federal University\" which will have a place_id of \"ChIJ9e_QQm0sDogRhUPatldEFxw\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "def find_KeyId(place):\n",
    "    \n",
    "    api_key = False\n",
    "    # If you have a Google Places API key, enter it here\n",
    "    # api_key = 'AIzaSy___IDByT70'\n",
    "    # https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n",
    "    if api_key is False:\n",
    "        api_key = 42\n",
    "        serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "    else :\n",
    "        serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "    # Ignore SSL certificate errors\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "    \n",
    "    address = place\n",
    "    if len(address) < 1: \n",
    "        print('error')\n",
    "        \n",
    "\n",
    "    parms = dict()\n",
    "    parms['address'] = address\n",
    "    \n",
    "    if api_key is not False: \n",
    "        parms['key'] = api_key\n",
    "    \n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "    print('Retrieving', url)\n",
    "        \n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "                   \n",
    "            \n",
    "    return print('The Key ID of the given input ' + place + \" is \" + str(js['results'][0]['place_id']))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://py4e-data.dr-chuck.net/json?address=South+Federal+University&key=42\n",
      "Retrieved 2291 characters\n",
      "The Key ID of the given input South Federal University is ChIJ9e_QQm0sDogRhUPatldEFxw\n",
      "Retrieving http://py4e-data.dr-chuck.net/json?address=K-State&key=42\n",
      "Retrieved 1807 characters\n",
      "The Key ID of the given input K-State is ChIJSXQyV43NvYcRdRt537z5Zg0\n"
     ]
    }
   ],
   "source": [
    "# this is a sample question\n",
    "place = 'South Federal University'\n",
    "find_KeyId(place)\n",
    "\n",
    "# this is a test question\n",
    "place = 'K-State'\n",
    "find_KeyId(place)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
